lm-eval --model "causal-ul2" --model_args="size=1549,mode='r'" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-r-0shot/" --log_samples --write_out --seed 1234 --num_fewshot=0
lm-eval --model "causal-ul2" --model_args="size=1549,mode='c'" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-c-0shot/" --log_samples --write_out --seed 1234 --num_fewshot=0
lm-eval --model "causal-ul2" --model_args="size=1549,mode='r'" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-r-1shot/" --log_samples --write_out --seed 2345 --num_fewshot=1
lm-eval --model "causal-ul2" --model_args="size=1549,mode='c'" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-c-1shot/" --log_samples --write_out --seed 2345 --num_fewshot=1
lm-eval --model "causal-ul2" --model_args="size=1549,mode='r'" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-r-2shot/" --log_samples --write_out --seed 3456 --num_fewshot=2
lm-eval --model "causal-ul2" --model_args="size=1549,mode='c'" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-c-2shot/" --log_samples --write_out --seed 3456 --num_fewshot=2