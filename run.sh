lm-eval --model "causal-ul2" --model_args="size=1549,mode=r" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-r-0shot/" --log_samples --write_out --seed 1234 --num_fewshot=0
lm-eval --model "causal-ul2" --model_args="size=1549,mode=c" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-c-0shot/" --log_samples --write_out --seed 1234 --num_fewshot=0
lm-eval --model "causal-ul2" --model_args="size=1549,mode=r" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-r-3shot/" --log_samples --write_out --seed 2345 --num_fewshot=3
lm-eval --model "causal-ul2" --model_args="size=1549,mode=c" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-c-3shot/" --log_samples --write_out --seed 2345 --num_fewshot=3
lm-eval --model "causal-ul2" --model_args="size=1549,mode=r" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-r-6shot/" --log_samples --write_out --seed 3456 --num_fewshot=6
lm-eval --model "causal-ul2" --model_args="size=1549,mode=c" --tasks="agieval_en,arithmetic,blimp,commonsense_qa,coqa,fda,glue,gpqa,hellaswag,headqa,ifeval,lambada,mmlu,piqa,truthfulqa,unscramble" --batch_size="auto" --output_path="results-c-6shot/" --log_samples --write_out --seed 3456 --num_fewshot=6